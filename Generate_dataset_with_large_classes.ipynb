{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6091f535-2467-477a-b496-dc206f18f9e6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m973.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Collecting huggingface-hub<1.0.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-2.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2.28.1)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m701.2/701.2 kB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-12.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.0/39.0 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets) (1.23.3)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (4.64.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (266 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.9/266.9 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.3/161.3 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.3.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2022.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, tzdata, pyyaml, pyarrow, multidict, fsspec, frozenlist, filelock, dill, async-timeout, yarl, responses, pandas, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 filelock-3.12.0 frozenlist-1.3.3 fsspec-2023.5.0 huggingface-hub-0.14.1 multidict-6.0.4 multiprocess-0.70.14 pandas-2.0.2 pyarrow-12.0.0 pyyaml-6.0 responses-0.18.0 tzdata-2023.3 xxhash-3.2.0 yarl-1.9.2\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting scipy>=1.3.2\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.8/site-packages (from scikit-learn) (1.23.3)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.2.2 scipy-1.10.1 threadpoolctl-3.1.0\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m804.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2023.5.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m771.9/771.9 kB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.23.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.12.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.11)\n",
      "Installing collected packages: tokenizers, regex, transformers\n",
      "Successfully installed regex-2023.5.5 tokenizers-0.13.3 transformers-4.29.2\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (2.0.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /opt/conda/lib/python3.8/site-packages (from pandas) (1.23.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas) (2022.2.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install scikit-learn\n",
    "!pip install openpyxl\n",
    "!pip install transformers\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41f7aab9-9dd1-4079-8305-2a597b447489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from sklearn import preprocessing\n",
    "from typing import List, Dict\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import random\n",
    "import collections\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "def set_random_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    # torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfccca41-4b35-46f5-bcf7-82d85e8595f6",
   "metadata": {},
   "source": [
    "### Generate 10 versions of each dataset as changing the random_seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1ae8b55-df41-4344-b9a3-d8d200acbda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset banking77 (/home/qonejung/.cache/huggingface/datasets/banking77/default/1.1.0/9898c11f6afa9521953d2ef205667b527bad14ef9cab445d470f16240c8c8ec4)\n",
      "100%|██████████| 2/2 [00:00<00:00, 517.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train classes is 47\n",
      "# of test classes is 30\n",
      "OK: train dataset and test dataset are disjoint\n",
      "Making train data is done\n",
      "Making test data is done\n"
     ]
    }
   ],
   "source": [
    "# Dataset: bank77\n",
    "\n",
    "OUTPUT_DIR = './data'\n",
    "DATA_NM = 'bank'\n",
    "\n",
    "def get_jsonl_data(jsonl_path: str):\n",
    "    assert jsonl_path.endswith(\".jsonl\")\n",
    "    out = list()\n",
    "    with open(jsonl_path, 'r', encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            j = json.loads(line.strip())\n",
    "            out.append(j)\n",
    "    return out\n",
    "\n",
    "def raw_data_to_dict(data, shuffle=True):\n",
    "    labels_dict = collections.defaultdict(list)\n",
    "    for item in data:\n",
    "        labels_dict[item['label']].append(item)\n",
    "    labels_dict = dict(labels_dict)\n",
    "    if shuffle:\n",
    "        for key, val in labels_dict.items():\n",
    "            random.shuffle(val)\n",
    "    return labels_dict\n",
    "\n",
    "def dir_check(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    if not os.path.exists(path+'/full'):\n",
    "        os.makedirs(path+'/full')\n",
    "\n",
    "\n",
    "def get_pandas_data(df):\n",
    "    out = list()\n",
    "    for idx in df.index:\n",
    "        out.append({\n",
    "            \"global_ix\" : idx,\n",
    "            \"sentence\": df.loc[idx, 'text'],\n",
    "            \"label\": str(df.loc[idx, 'label'])\n",
    "        })\n",
    "    return out\n",
    "\n",
    "def write_jsonl_data(jsonl_data: List[Dict], jsonl_path: str, force=False):\n",
    "    if os.path.exists(jsonl_path) and not force:\n",
    "        raise FileExistsError\n",
    "    with open(jsonl_path, 'w') as file:\n",
    "        for line in jsonl_data:\n",
    "            file.write(json.dumps(line, ensure_ascii=False) + '\\n')\n",
    "\n",
    "def write_full_txt_data(jsonl_data: List[Dict], txt_path: str, force=False):\n",
    "    if os.path.exists(txt_path) and not force:\n",
    "        raise FileExistsError\n",
    "    with open(txt_path, 'w') as file:\n",
    "        for line in jsonl_data:\n",
    "            file.write(line['sentence'] + '\\n')\n",
    "\n",
    "            \n",
    "\n",
    "dataset = load_dataset(\"banking77\")\n",
    "\n",
    "train_df = pd.DataFrame(dataset[\"train\"])\n",
    "test_df = pd.DataFrame(dataset[\"test\"])\n",
    "df = pd.concat([train_df, test_df],ignore_index=True)\n",
    "\n",
    "train_classes = list(range(77))\n",
    "eval_classes = random.sample(train_classes,30)\n",
    "\n",
    "for c in eval_classes:\n",
    "    train_classes.remove(c)\n",
    "\n",
    "train_df = df.loc[df['label'].isin(train_classes)]\n",
    "eval_df = df.loc[df['label'].isin(eval_classes)]\n",
    "\n",
    "train_cls = set(train_df['label'])\n",
    "eval_cls = set(eval_df['label'])\n",
    "print(f\"# of train classes is {len(train_cls)}\")\n",
    "print(f\"# of test classes is {len(eval_cls)}\")\n",
    "\n",
    "if len(eval_cls.intersection(train_cls)) == 0 and len(train_cls.intersection(eval_cls)) == 0:\n",
    "    print(\"OK: train dataset and test dataset are disjoint\")\n",
    "else:\n",
    "    print(\"WARN: train dataset and test dataset are not disjoint\")\n",
    "\n",
    "dir_check(OUTPUT_DIR + '/' +DATA_NM + f'/{str(rs)}')\n",
    "\n",
    "out = get_pandas_data(train_df)\n",
    "write_jsonl_data(out, OUTPUT_DIR + '/' +DATA_NM +'/train.jsonl', force=True)\n",
    "write_full_txt_data(out, OUTPUT_DIR + '/' +DATA_NM +'/full/full-train.txt', force=True)\n",
    "print(\"Making train data is done\")\n",
    "\n",
    "\n",
    "out = get_pandas_data(eval_df)\n",
    "write_jsonl_data(out, OUTPUT_DIR + '/' +DATA_NM +'/test.jsonl', force=True)\n",
    "write_full_txt_data(out, OUTPUT_DIR + '/' +DATA_NM +'/full/full-test.txt', force=True)\n",
    "print(\"Making test data is done\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "25088782-9ebd-4b5d-9d8a-521bc490b75d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train classes is 63\n",
      "# of train classes is 30\n",
      "WARN: train dataset and test dataset are not disjoint\n",
      "Making train data is done\n",
      "Making test data is done\n"
     ]
    }
   ],
   "source": [
    "# Dataset: medium post title -93\n",
    "# source: \n",
    "OUTPUT_DIR = './data'\n",
    "DATA_NM = 'medium'\n",
    "\n",
    "def dir_check(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    if not os.path.exists(path+'/full'):\n",
    "        os.makedirs(path+'/full')\n",
    "\n",
    "\n",
    "def get_pandas_data(df):\n",
    "    out = list()\n",
    "    for idx in df.index:\n",
    "        out.append({\n",
    "            \"global_ix\" : idx,\n",
    "            \"sentence\": df.loc[idx, 'title'],\n",
    "            \"label\": str(df.loc[idx, 'category'])\n",
    "        })\n",
    "    return out\n",
    "\n",
    "def write_jsonl_data(jsonl_data: List[Dict], jsonl_path: str, force=False):\n",
    "    if os.path.exists(jsonl_path) and not force:\n",
    "        raise FileExistsError\n",
    "    with open(jsonl_path, 'w') as file:\n",
    "        for line in jsonl_data:\n",
    "            file.write(json.dumps(line, ensure_ascii=False) + '\\n')\n",
    "\n",
    "def write_full_txt_data(jsonl_data: List[Dict], txt_path: str, force=False):\n",
    "    if os.path.exists(txt_path) and not force:\n",
    "        raise FileExistsError\n",
    "    with open(txt_path, 'w') as file:\n",
    "        for line in jsonl_data:\n",
    "            file.write(line['sentence'] + '\\n')\n",
    "\n",
    "            \n",
    "df = pd.read_csv('./raw_data/medium_post_titles.csv')\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df.category)\n",
    "df['category'] = le.transform(df.category)\n",
    "\n",
    "train_classes = list(range(93))\n",
    "eval_classes = random.sample(train_classes,30)\n",
    "\n",
    "for c in eval_classes:\n",
    "    train_classes.remove(c)\n",
    "\n",
    "train_df = df.loc[df['category'].isin(train_classes)]\n",
    "eval_df = df.loc[df['category'].isin(eval_classes)]\n",
    "\n",
    "train_cls = set(train_df['category'])\n",
    "eval_cls = set(eval_df['category'])\n",
    "print(f\"# of train classes is {len(train_cls)}\")\n",
    "print(f\"# of test classes is {len(eval_cls)}\")\n",
    "if len(eval_cls.intersection(train_cls)) == 0 and len(train_cls.intersection(eval_cls)) == 0:\n",
    "    print(\"OK: train dataset and test dataset are disjoint\")\n",
    "else:\n",
    "    print(\"WARN: train dataset and test dataset are not disjoint\")\n",
    "    \n",
    "dir_check(OUTPUT_DIR + '/' +DATA_NM)\n",
    "    \n",
    "out = get_pandas_data(train_df)\n",
    "write_jsonl_data(out, OUTPUT_DIR + '/' +DATA_NM +'/train.jsonl', force=True)\n",
    "write_full_txt_data(out, OUTPUT_DIR + '/' +DATA_NM +'/full/full-train.txt', force=True)\n",
    "print(\"Making train data is done\")\n",
    "\n",
    "\n",
    "out = get_pandas_data(eval_df)\n",
    "write_jsonl_data(out, OUTPUT_DIR + '/' +DATA_NM +'/test.jsonl', force=True)\n",
    "write_full_txt_data(out, OUTPUT_DIR + '/' +DATA_NM +'/full/full-test.txt', force=True)\n",
    "print(\"Making test data is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "96ff8e41-64a7-4804-b792-ffb6ee2f7751",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train classes is 103\n",
      "# of train classes is 30\n",
      "OK: train dataset and test dataset are disjoint\n",
      "Making train data is done\n",
      "Making test data is done\n"
     ]
    }
   ],
   "source": [
    "# Dataset: 2020 r&d project dataset  - 133\n",
    "# source: ntis\n",
    "OUTPUT_DIR = './data'\n",
    "DATA_NM = 'rnd'\n",
    "\n",
    "def dir_check(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    if not os.path.exists(path+'/full'):\n",
    "        os.makedirs(path+'/full')\n",
    "\n",
    "\n",
    "def get_pandas_data(df):\n",
    "    out = list()\n",
    "    for idx in df.index:\n",
    "        out.append({\n",
    "            \"global_ix\" : idx,\n",
    "            \"sentence\": df.loc[idx, 'ENG_PJT_NM'],\n",
    "            \"label\": str(df.loc[idx, 'RSCH_AREA_CLS1_CD'])\n",
    "        })\n",
    "    return out\n",
    "\n",
    "def write_jsonl_data(jsonl_data: List[Dict], jsonl_path: str, force=False):\n",
    "    if os.path.exists(jsonl_path) and not force:\n",
    "        raise FileExistsError\n",
    "    with open(jsonl_path, 'w') as file:\n",
    "        for line in jsonl_data:\n",
    "            file.write(json.dumps(line, ensure_ascii=False) + '\\n')\n",
    "\n",
    "def write_full_txt_data(jsonl_data: List[Dict], txt_path: str, force=False):\n",
    "    if os.path.exists(txt_path) and not force:\n",
    "        raise FileExistsError\n",
    "    with open(txt_path, 'w') as file:\n",
    "        for line in jsonl_data:\n",
    "            file.write(line['sentence'] + '\\n')\n",
    "\n",
    "            \n",
    "df = pd.read_excel('./raw_data/sh_pjt_en_2020.xlsx',engine='openpyxl')\n",
    "df = df[df['RSCH_AREA_CLS1_CD'].map(df['RSCH_AREA_CLS1_CD'].value_counts()) > 50]\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df.RSCH_AREA_CLS1_CD)\n",
    "df['RSCH_AREA_CLS1_CD'] = le.transform(df.RSCH_AREA_CLS1_CD)\n",
    "\n",
    "train_classes = list(range(133))\n",
    "eval_classes = random.sample(train_classes,30)\n",
    "\n",
    "for c in eval_classes:\n",
    "    train_classes.remove(c)\n",
    "\n",
    "train_df = df.loc[df['RSCH_AREA_CLS1_CD'].isin(train_classes)]\n",
    "eval_df = df.loc[df['RSCH_AREA_CLS1_CD'].isin(eval_classes)]\n",
    "\n",
    "train_cls = set(train_df['RSCH_AREA_CLS1_CD'])\n",
    "eval_cls = set(eval_df['RSCH_AREA_CLS1_CD'])\n",
    "print(f\"# of train classes is {len(train_cls)}\")\n",
    "print(f\"# of train classes is {len(eval_cls)}\")\n",
    "if len(eval_cls.intersection(train_cls)) == 0 and len(train_cls.intersection(eval_cls)) == 0:\n",
    "    print(\"OK: train dataset and test dataset are disjoint\")\n",
    "else:\n",
    "    print(\"WARN: train dataset and test dataset are not disjoint\")\n",
    "    \n",
    "dir_check(OUTPUT_DIR + '/' +DATA_NM)\n",
    "    \n",
    "out = get_pandas_data(train_df)\n",
    "write_jsonl_data(out, OUTPUT_DIR + '/' +DATA_NM +'/train.jsonl', force=True)\n",
    "write_full_txt_data(out, OUTPUT_DIR + '/' +DATA_NM +'/full/full-train.txt', force=True)\n",
    "print(\"Making train data is done\")\n",
    "\n",
    "\n",
    "out = get_pandas_data(eval_df)\n",
    "write_jsonl_data(out, OUTPUT_DIR + '/' +DATA_NM +'/test.jsonl', force=True)\n",
    "write_full_txt_data(out, OUTPUT_DIR + '/' +DATA_NM +'/full/full-test.txt', force=True)\n",
    "print(\"Making test data is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f50bd62-8e71-4a91-afb7-88f5b6103643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: web of science   \n",
    "#load_dataset(\"web_of_science\",'WOS46985')\n",
    "\n",
    "OUTPUT_DIR = './data'\n",
    "DATA_NM = 'wos'\n",
    "\n",
    "def dir_check(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    if not os.path.exists(path+'/full'):\n",
    "        os.makedirs(path+'/full')\n",
    "\n",
    "\n",
    "def get_pandas_data(df):\n",
    "    out = list()\n",
    "    for idx in df.index:\n",
    "        out.append({\n",
    "            \"global_ix\" : idx,\n",
    "            \"sentence\": df.loc[idx, 'input_data'],\n",
    "            \"label\": str(df.loc[idx, 'label'])\n",
    "        })\n",
    "    return out\n",
    "\n",
    "def write_jsonl_data(jsonl_data: List[Dict], jsonl_path: str, force=False):\n",
    "    if os.path.exists(jsonl_path) and not force:\n",
    "        raise FileExistsError\n",
    "    with open(jsonl_path, 'w') as file:\n",
    "        for line in jsonl_data:\n",
    "            file.write(json.dumps(line, ensure_ascii=False) + '\\n')\n",
    "\n",
    "def write_full_txt_data(jsonl_data: List[Dict], txt_path: str, force=False):\n",
    "    if os.path.exists(txt_path) and not force:\n",
    "        raise FileExistsError\n",
    "    with open(txt_path, 'w') as file:\n",
    "        for line in jsonl_data:\n",
    "            file.write(line['sentence'] + '\\n')\n",
    "\n",
    "            \n",
    "\n",
    "dataset = load_dataset(\"web_of_science\",'WOS46985')\n",
    "\n",
    "df = pd.DataFrame(dataset[\"train\"])\n",
    "\n",
    "\n",
    "train_classes = list(set(df.label))\n",
    "eval_classes = random.sample(train_classes,30)\n",
    "\n",
    "for c in eval_classes:\n",
    "    train_classes.remove(c)\n",
    "\n",
    "train_df = df.loc[df['label'].isin(train_classes)]\n",
    "eval_df = df.loc[df['label'].isin(eval_classes)]\n",
    "\n",
    "train_cls = set(train_df['label'])\n",
    "eval_cls = set(eval_df['label'])\n",
    "print(f\"# of train classes is {len(train_cls)}\")\n",
    "print(f\"# of test classes is {len(eval_cls)}\")\n",
    "if len(eval_cls.intersection(train_cls)) == 0 and len(train_cls.intersection(eval_cls)) == 0:\n",
    "    print(\"OK: train dataset and test dataset are disjoint\")\n",
    "else:\n",
    "    print(\"WARN: train dataset and test dataset are not disjoint\")\n",
    "    \n",
    "dir_check(OUTPUT_DIR + '/' +DATA_NM)\n",
    "    \n",
    "out = get_pandas_data(train_df)\n",
    "write_jsonl_data(out, OUTPUT_DIR + '/' +DATA_NM +'/train.jsonl', force=True)\n",
    "write_full_txt_data(out, OUTPUT_DIR + '/' +DATA_NM +'/full/full-train.txt', force=True)\n",
    "print(\"Making train data is done\")\n",
    "\n",
    "\n",
    "out = get_pandas_data(eval_df)\n",
    "write_jsonl_data(out, OUTPUT_DIR + '/' +DATA_NM +'/test.jsonl', force=True)\n",
    "write_full_txt_data(out, OUTPUT_DIR + '/' +DATA_NM +'/full/full-test.txt', force=True)\n",
    "print(\"Making test data is done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
